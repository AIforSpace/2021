---
layout: project
urltitle:  "Learning 3D Generative Models"
title: "Learning 3D Generative Models"
categories: cvpr, workshop, computer vision, computer graphics, deep learning, generative modeling, visual learning, simulation environments, robotics, machine learning, reinforcement learning
permalink: /
favicon: /static/img/ico/favicon.png
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row header-row">
  <div class="col-xs-12 header-img">  
    <center><h1>Learning 3D Generative Models</h1></center>
    <center><h2>CVPR 2020 Workshop, Seattle, WA</h2></center>
    <center><span style="font-weight:400;">14th of June 2020</span></center>
    <!--<center><span style="color:#e74c3c;font-weight:400;">Time and Location TBD</span></center>-->
    <br/>
  </div>
</div>

<hr>

<!-- <div class="row" id="">
  <div class="col-md-12">
    <img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}">
    <p> Image credit: [1, 2, 7, 12, 6, 4, 5]</p>
  </div>
</div> -->

<br>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      The past several years have seen an explosion of interest in generative modeling: unsupervised models which learn to synthesize new elements from the training data domain. Such models have been used to breathtaking effect for generating realistic images, especially of human faces, which are in some cases indistinguishable from reality. The unsupervised latent representations learned by these models can also prove powerful when used as feature sets for supervised learning tasks.
    </p>
    <p>
      Thus far, the vision community's attention has mostly focused on generative models of 2D images. However, in computer graphics, there has been a recent surge of activity in generative models of three-dimensional content: learnable models which can synthesize novel 3D objects, or even larger scenes composed of multiple objects. As the vision community turns from passive internet-images based vision toward more <i>embodied</i> vision tasks, these kinds of 3D generative models become increasingly important: as unsupervised feature learners, as training data synthesizers, as a platform to study 3D representations for 3D vision tasks, and as a way of equipping an embodied agent with a 3D `imagination' about the kinds of objects and scenes it might encounter.
    </p>
    <p>
     With this workshop, we aim to bring together researchers working on generative models of 3D shapes and scenes with researchers and practitioners who can use these generative models to improve embodied vision tasks. For our purposes, we define ``generative model'' to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. images), language, or other high-level specifications. Vision tasks that can benefit from such models include scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, question answering, and more.
    </p>
  </div>
</div> <br>   

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <span style="font-weight:500;">Call for papers:</span> We invite novel full papers of 4 to 6 pages (extended abstracts are not allowed) for work on tasks related to data-driven 3D generative modeling or tasks leveraging generated 3D content.
      Paper topics may include but are not limited to:
    </p>
    <ul>
      <li>Generative models for 3D shape and 3D scene synthesis</li>
      <li>Generating 3D shapes and scenes from real world data (images, videos, or scans)</li>
      <li>Representations for 3D shapes and scenes</li>
      <li>Unsupervised feature learning for embodied vision tasks via 3D generative models</li>
      <li>Training data synthesis/augmentation for embodied vision tasks via 3D generative models</li>
    </ul>
    <p>
      <span style="font-weight:500;">Submission:</span> we encourage submissions of up to 6 pages excluding references and acknowledgements.
      The submission should be in the CVPR format.
      Reviewing will be single blind.
      Accepted works will be published in the CVPR 2020 proceedings (online/app, IEEE Xplore, and CVF open access).
      Due to the archival nature of these publications, we are looking for work that has not been published before.
      The submissions will be handled by the <a href='https://cmt3.research.microsoft.com/L3DGM2020'>CMT paper management system</a>. 
      <!--Please submit your paper to the following address by the deadline: <span style="color:#1a1aff;font-weight:400;"><a href="mailto:learn3dgen@gmail.com">learn3dgen@gmail.com</a></span>-->
    </p>
  </div>
</div><br>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>March 30 2020 - AoE time (UTC -12)</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>April 13 2020</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>April 20 2020</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>June 14 2020</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>


<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
    <p>
      TBA.
    </p>
  </div>
</div>


<br>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
    <p>
      TBA.
    </p>
  </div>
</div>


<br>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>


<!-- 26 -->
<div class="row">
  <div class="col-md-12">
    <a href="https://people.cs.umass.edu/~kalo/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/vangelis.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a></b> is an Associate Professor Computer Science at the University of Massachussetts Amherst. His research deals with the development of visual computing and machine learning techniques that help people to easily create and process representations of the 3D visual world, including 3D models of objects and scenes, 3D scans, animations, shape collections, and images. His research is supported by NSF awards and donations from Adobe. He was a postdoctoral researcher at Stanford University from 2010 to 2012 (advised by Leo Guibas and Vladlen Koltun). He obtained his PhD from the University of Toronto in 2010 (advised by Aaron Hertzmann and Karan Singh). He graduated from the department of Electrical and Computer Engineering, Technical University of Crete in 2005 (undergraduate thesis advised by Stavros Christodoulakis).
    </p>
  </div>
</div><br>

<!-- 24 -->
<div class="row">
  <div class="col-md-12">
    <a href="https://jiajunwu.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/jiajun.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://jiajunwu.com/">Jiajun Wu</a></b> is a Visiting Faculty Researcher at Google Research, New York City, working with Noah Snavely. In Fall 2020, He will join Stanford University as an Assistant Professor of Computer Science. He studies machine perception, reasoning, and its interaction with the physical world, drawing inspiration from human cognition. He completed my PhD at MIT, advised by Bill Freeman and Josh Tenenbaum, and his undergraduate degrees from Tsinghua University, working with Zhuowen Tu. He has also spent time at the research labs of Microsoft, Facebook, and Baidu.
    </p>
  </div>
</div><br>

<!-- 23 -->
<div class="row">
  <div class="col-md-12">
    <a href="http://www.vovakim.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/vova.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://www.vovakim.com/">Vladimir Kim</a></b> is a Senior Research Scientist at Adobe Research Seattle. He works on geometry analysis algorithms at the intersection of graphics, vision, and machine learning, enabling novel interfaces for creative tasks. His recent research focuses on making it easier to understand, model, manipulate, and process geometric data such as models of 3D objects, interior environments, articulated characters, and fonts.
    </p>
  </div>
</div><br>

<!-- 16 -->
<div class="row">
  <div class="col-md-12">
    <a href=""><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/georgia.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://gkioxari.github.io/">Georgia Gkioxari</a></b> is a research scientist at FAIR. She received her PhD from UC Berkeley, where she was advised by Jitendra Malik. She did her bachelors in ECE at NTUA in Athens, Greece, where she worked with Petros Maragos. In the past, she has spent time at Google Brain and Google Research, where she worked with Navdeep Jaitly and Alexander Toshev.
    </p>
  </div>
</div><br>




<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">

  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/daniel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://fgolemo.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/florian.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://fgolemo.github.io/">Florian Golemo</a>
      <h6>MILA, Element AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/angel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cse.iitb.ac.in/~sidch/">
      <img class="people-pic" src="{{ "/static/img/people/sid.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>
      <h6>Adobe Research, IIT Bombay</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://mila.quebec/en/person/aaron-courville/">
      <img class="people-pic" src="{{ "/static/img/people/aaron.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://mila.quebec/en/person/aaron-courville/">Aaron Courville</a>
      <h6>MILA</h6>
    </div>
  </div>


  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~huangqx/">
      <img class="people-pic" src="{{ "/static/img/people/qixing.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~huangqx/">Qixing Huang</a>
      <h6>UT Austin</h6>
    </div>
  </div>

</div>


<hr>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %}

<br>

